# Training Requirements for MCP Tool Specialist Models
# Fine-tuning Qwen 2.5 Coder 32B with LoRA for tool calling

# Core ML/Training
torch>=2.1.0
transformers>=4.36.0
peft>=0.7.0  # LoRA implementation
accelerate>=0.25.0
bitsandbytes>=0.41.0  # For quantization (optional)

# Data Processing
datasets>=2.15.0
tokenizers>=0.15.0

# Training Utilities
trl>=0.7.0  # Transformer Reinforcement Learning (optional)
wandb>=0.16.0  # Experiment tracking (optional)
tensorboard>=2.15.0  # Alternative to wandb

# Evaluation
scikit-learn>=1.3.0
numpy>=1.24.0
pandas>=2.1.0

# Model Loading & Optimization
sentencepiece>=0.1.99  # For tokenizer
safetensors>=0.4.0  # Fast model loading
einops>=0.7.0  # Tensor operations

# Utilities
tqdm>=4.66.0
pyyaml>=6.0
python-dotenv>=1.0.0

# Optional: For distributed training
deepspeed>=0.12.0  # Optional, for multi-GPU

# Optional: For model quantization
auto-gptq>=0.5.0  # Optional, for inference optimization
